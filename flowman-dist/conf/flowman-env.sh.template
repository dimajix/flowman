#!/usr/bin/env bash

# Specify Java home (just in case)
#
#export JAVA_HOME

# Explicitly override Flowmans home. These settings are detected automatically,
# but can be overridden
#
#export FLOWMAN_HOME
#export FLOWMAN_CONF_DIR

# Set URL of Flowman Studio server as used by flowctl
#export FLOWMAN_STUDIO_URL

# Specify any environment settings and paths
#
#export SPARK_HOME
#export HADOOP_HOME
#export HADOOP_CONF_DIR=${HADOOP_CONF_DIR=$HADOOP_HOME/conf}
#export YARN_HOME
#export HDFS_HOME
#export MAPRED_HOME
#export HIVE_HOME
#export HIVE_CONF_DIR=${HIVE_CONF_DIR=$HIVE_HOME/conf}

# Set the Kerberos principal in YARN cluster
#
#KRB_PRINCIPAL=
#KRB_KEYTAB=

# Specify the YARN queue to use
#
#YARN_QUEUE=

# Specify Spark master
#
#SPARK_MASTER=

# Resource configuration. This probably should be overridden either in the application or via the corresponding
# Spark config variables in an "environment" section.
#
SPARK_EXECUTOR_CORES="4"
SPARK_EXECUTOR_MEMORY="8G"
SPARK_DRIVER_CORES="2"
SPARK_DRIVER_MEMORY="3G"

# Specify additional Spark options to be passed to spark-submit
#
#SPARK_OPTS=

# Use a different spark-submit (for example spark2-submit in Cloudera)
#
#SPARK_SUBMIT=


# Apply any proxy settings from the system environment
#
http_proxy_host=$(echo $http_proxy | sed 's#.*//\([^:]*\).*#\1#') && \
http_proxy_port=$(echo $http_proxy | sed 's#.*//[^:]*:\([0-9]*\)#\1#') && \
if [[ "$http_proxy_host" != "" ]]; then
    SPARK_DRIVER_JAVA_OPTS="-Dhttp.proxyHost=${http_proxy_host} -Dhttp.proxyPort=${http_proxy_port} $SPARK_DRIVER_JAVA_OPTS"
    SPARK_EXECUTOR_JAVA_OPTS="-Dhttp.proxyHost=${http_proxy_host} -Dhttp.proxyPort=${http_proxy_port} $SPARK_EXECUTOR_JAVA_OPTS"
    SPARK_OPTS="--conf spark.hadoop.fs.s3a.proxy.host=${http_proxy_host} --conf spark.hadoop.fs.s3a.proxy.port=${http_proxy_port} $SPARK_OPTS"
fi

https_proxy_host=$(echo $https_proxy | sed 's#.*//\([^:]*\).*#\1#') && \
https_proxy_port=$(echo $https_proxy | sed 's#.*//[^:]*:\([0-9]*\)#\1#') && \
if [[ "$https_proxy_host" != "" ]]; then
    SPARK_DRIVER_JAVA_OPTS="-Dhttps.proxyHost=${https_proxy_host} -Dhttps.proxyPort=${https_proxy_port} $SPARK_DRIVER_JAVA_OPTS"
    SPARK_EXECUTOR_JAVA_OPTS="-Dhttps.proxyHost=${https_proxy_host} -Dhttps.proxyPort=${https_proxy_port} $SPARK_EXECUTOR_JAVA_OPTS"
fi

# Set AWS credentials if required. You can also specify these in project config
#
if [[ "$AWS_ACCESS_KEY_ID" != "" ]]; then
    SPARK_OPTS="--conf spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID} --conf spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY} $SPARK_OPTS"
fi
