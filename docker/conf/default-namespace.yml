name: "default"

history:
  kind: jdbc
  connection: flowman_state
  retries: 3
  timeout: 1000

connections:
  flowman_state:
    driver: $System.getenv('FLOWMAN_LOGDB_DRIVER', 'org.apache.derby.jdbc.EmbeddedDriver')
    url: $System.getenv('FLOWMAN_LOGDB_URL', $String.concat('jdbc:derby:', $System.getenv('FLOWMAN_HOME'), '/var/history;create=true'))
    username: $System.getenv('FLOWMAN_LOGDB_USER', '')
    password: $System.getenv('FLOWMAN_LOGDB_PASSWORD', '')

# This adds a hook for creating an execution log in a file
hooks:
  kind: report
  location: ${project.basedir}/generated-report.txt
  metrics:
    # Define common labels for all metrics
    labels:
      project: ${project.name}
      phase: ${phase}
    metrics:
      # Collect everything
      - selector:
          name: .*
        labels:
          category: ${category}
          kind: ${kind}
          name: ${name}

# This configures where metrics should be written to. Since we cannot assume a working Prometheus push gateway, we
# simply print them onto the console
metrics:
  - kind: console

config:
  - spark.sql.warehouse.dir=/opt/flowman/var/hive/warehouse
  - spark.hadoop.hive.metastore.uris=
  - spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/opt/flowman/var/hive/db;create=true
  - spark.hadoop.datanucleus.rdbms.datastoreAdapterClassName=org.datanucleus.store.rdbms.adapter.DerbyAdapter

store:
  kind: file
  location: $System.getenv('FLOWMAN_HOME')/examples

plugins:
  - flowman-avro
  - flowman-aws
  - flowman-azure
  - flowman-delta
  - flowman-kafka
  - flowman-mariadb
  - flowman-mysql
  - flowman-mssqlserver
  - flowman-oracle
  - flowman-postgresql
  - flowman-swagger
  - flowman-openapi
  - flowman-json
